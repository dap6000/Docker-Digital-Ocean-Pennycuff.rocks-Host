<!DOCTYPE html>
<html>
<head>
	<title>Intro to Docker</title>
	<link rel="stylesheet" href="default.min.css">
	<style type="text/css">
		h1 { text-align: center; }
		h2 { text-align: center; }
		h3 { text-align: center; }
	</style>
	<script src="highlight.min.js"></script>
	<script src="jquery-1.8.2.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<script>
		var slideshow = function() {
			var $divs = $('body > div');
			$divs.hide();
			var $currentPage = $divs.first();
			$currentPage.show();

			var slideMode = true;

			var nextSlide = function () {
				if($currentPage.next().length == 0) { return; }
				$currentPage.hide();
				$currentPage = $currentPage.next();
				$currentPage.show();
			};

			var prevSlide = function () {
				if($currentPage.prev().length == 0) { return; }
				$currentPage.hide();
				$currentPage = $currentPage.prev();
				$currentPage.show();
			}

			var x = 0;
			var y = 0;
			var dx = 0;
			var dy = 0;
			var started = false;
			var touchStart = function(evt) {
				started = true;
				x = evt.touches[0].clientX;
				y = evt.touches[0].clientY;
			};

			var touchMove = function(evt) {
				if(!started) { return; }
				var nx = evt.touches[0].clientX;
				var ny  = evt.touches[0].clientY;
				dx = x - nx;
				dy = y - ny;
			};

			var touchEnd = function(evt) {
				started = false;
				if (Math.abs(dx) > (1.8 * Math.abs(dy))) {
					if(dx > 0) {
						nextSlide();
					} else {
						prevSlide();
					}
				}
			}

			document.addEventListener('touchstart', touchStart, false);        
			document.addEventListener('touchmove', touchMove, false);
			document.addEventListener('touchend', touchEnd, false);

			$('body').keydown(function (evt) {
				if(evt.keyCode == 27) {
					if(slideMode) {
						slideMode = false;
						$divs.show();
					} else {
						slideMode = true;
						$divs.hide();
						$currentPage = $divs.first();
						$currentPage.show();
					}
				} else if(slideMode && (evt.keyCode == 37)) {
					// left
					prevSlide();
				} else if(slideMode && (evt.keyCode == 39)) {
					// right
					nextSlide();
				}
			});
		};
		$(document).ready(function () {
			slideshow();
		});
</script>
</head>
<body>

<div class="side">
	<h1>An Introduction to Docker</h1>
	<h2>Emily Estes</h2>
	<h3>2016-09-26 @ DevOps Knoxville Meetup</h3>
	<p>To navigate: left arrow - previous slide, right arrow - next slide, escape - toggle slide show mode.</p>
</div>



<div class="side">
<h1>Goals of this presentation</h1>
<ul>
	<li>Explain what a container is</li>
	<li>Give an overview of the Docker ecosystem</li>
	<li>Demonstrate how to create a basic Docker container</li>
	<li>Explain some patterns that work for me in building, testing, and deploying applications with Docker</li>
	<li>Point out some non-devops usages of containers</li>
</ul>
</div>



<div class="slide">
	<h1>What is a container?</h1>
	<ul>
		<li>A way of making local variables seem like globals. - Emily Estes</li>
		<li>Static linking for servers. - Emily Estes</li>
	</ul>
</div>



<div class="slide">
	<h1>What is a container?</h1>
	<ul>
		<li>A way of making local variables seem like globals. - Emily Estes</li>
		<li>Static linking for servers. - Emily Estes</li>
	</ul>

	<h2>No... seriously what is a container?</h2>
	<p>A container "virtualizes" many aspects of a operating system to manage a group of processes giving them their own view of system resources, including memory, processes, I/O, networking, users, and the file system. On Linux in particular, it virtualizes the kernel, giving the appearance of having a whole operating system while sharing the computer with other groups of processes sharing the same kernel. If this sounds a lot like a virtual machine, you're not wrong. Virtual Machines like Xen, VMWare, and the JVM and others virtualize systems at the hardware layer. So philosophically, Docker is mostly nothing new. Docker is popularizing ideas from previous systems and was originally built as a way to conveniently use the LXC (Linux Containerization) package which is built on the cgroup API's that exist in the Linux kernel.</p>
</div>



<div class="slide">
<h1>A brief history of containers</h1>
<ul>
	<li>1979 - chroot was added to Unix Version 7 to "change the root" of the file system for a process and it's children.</li>
	<li>2000 - FreeBSD adds jail system call and associated utilities to virtualize processes as well as changing the root of the file system.</li>
	<li>2005 - Solaris Zones are added to Solaris 10 providing virtualization of the kernel to create virtual servers.</li>
	<li>2005 - OpenVZ creates an extended Linux kernel supporting virtualization similiar to Solaris Zones</li>
	<li>2007 - Linux adds cgroups to kernel, allowing groups of processes to be managed as a single unit and allowing their view of the system to be isolated from other processes.</li>
	<li>2008 - LXC is released making use of the cgroup syscalls added to Linux to create containers.</li>
	<li>2010 - illumos is released as a fork of OpenSolaris, and continues to encourage people to use Solaris Zones to virtualize servers.</li>
	<li>2013 - Docker is released which initially is a set of wrappers to simplify using LXC.</li>
	<li>2015 - The internet loses it mind about this brand new technology called containerization. /s</li>
</ul>
</div>



<div class="slide">
<h1>What is Docker</h1>
<p>Docker is a client/server program to create and manage containers running on a Linux server. The docker daemon is a server that has a list of tagged and versioned images(templates if you will) which are used to create image instances, i.e. containers. The docker client scripts the creation of images with a simple scripting language. In addition to the script(called the Dockerfile), you can include a collection of files that can be referenced in the script(This collection is called the "Build context"). The image created by docker is just a collection of files... if you use the docker client to export an image, you'll get a .tar file containing the filesystem used inside the container.</p>
</div>



<div class="slide">
<h1>Now forget all of that.</h1>
</div>



<div class="slide">
<h1>Now forget all of that.</h1>
<h2>Let's set up a basic web server with nginx and php.<h2>
</div>



<div class="slide">
<h1>Now forget all of that.</h1>
<h2>Let's set up a basic web server with nginx and php.</h2>
<p>If you were just going to set up a vanilla install of Debian (for example), once you were at a shell, a manual install would go something like the following sequence of commands:</p>
<pre><code class="bash">apt-get update
apt-get install -y nginx supervisor php5 php5-fpm php5-common
cp /my-nginx.conf /etc/nginx/nginx.conf # let's just assume we have a standard configuration of nginx copied from somewhere
cp /my-php-fpm.conf /etc/php5/fpm/php-fpm.conf # Again, we have a set of tweaked config files we want to use for PHP as well.
cp /my-supervisord.conf /etc/supervisord/conf.d/supervisord.conf # Also we want to use supervisord to manage this server.
mkdir -p /opt/example # The location we're going to install our app to.
cp /my-app/* /opt/example
chown www-data -R /opt/example/
chgrp www-data -R /opt/example/
supervisord -c /etc/supervisor/conf.d/supervisord.conf # start supervisord so it can start our servers.
</code></pre>

<p>Obviously, we need to have some config files stashed somewhere along with our example app. But this is roughly what you would do for a simple app. More complex apps would involve a similar process with more steps, but would follow the same basic ideas.</p>
</div>


<div class="slide">
<h1>So what's wrong with setting up a server this way?</h1>
<ul>
	<li>It's a manual process, so we could forget some steps.</li>
	<li>If there is a mistake in that list of commands, most of us would investigate the issue and run a tweaked command to get it to work... but we might forget to write down the correction.</li>
	<li>This is not (easily) repeatable.</li>
	<li>This is not automatable (as is).</li>
</ul>
<h2>Do you want a snowflake server? Because that's how you get a snowflake server.</h2>
</div>



<div class="slide">
<h1>An example Dockerfile</h1>
<p>Docker images are created using a script in a file named <em>Dockerfile</em>. The rest of the files (And folders) within the same folder the Dockerfile lives in are called the "Build context" and they are available for use within the Dockerfile. We can take the list of instructions for building a web server from the previous slide and turn them into a Dockerfile like so:
<pre><code class="dockerfile"># nginx/php Docker example.
FROM debian:jessie

RUN apt-get update &amp;&amp; \
    apt-get install -y nginx supervisor php5 php5-fpm php5-common

COPY ["/etc", "/etc/"]
COPY ["/opt", "/opt/"]
COPY ["/var", "/var/"]

RUN chown www-data -R /opt/example/ &amp;&amp; \
    chgrp www-data -R /opt/example/ &amp;&amp; \
    chmod a+rw /var/opt/example/ &amp;&amp; \
    chmod a+w /var/log/

EXPOSE 8080

CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]</code></pre>
In addition to the Dockerfile itself, there are some config files for various services in the same folder. My personal convention is to put them in the place they will go on the "final" install and just copy them over wholesale. (The config files I am using here are in the same git repo as the slides for this presentation.)</p>

<h3>Building and running...</h3>
<p>To build the image from this Dockerfile we run the following command:
<pre><code class="bash">docker build -t example .</code></pre>
This will take a few minutes (because of apt). Once it's done, you can run the image like so:
<pre><code class="bash">docker run -d --name="example-container" -p 8080:8080 example</code></pre>
Now you can browse to <a href="http://localhost:8080/index.php">http://localhost:8080/index.php</a> and see the web app running. In addition to this, if we want to inspect things going on inside the container, we can get a shell via the following command:
<pre><code class="bash">docker exec -it example-container bash</code></pre>
</p>
</div>



<div class="slide">
<h1>A digression about PID 1</h1>
<p>The root process in your container will be running as PID(process ID) 1. On *nix, PID 1 has some special responsibilities, namely reaping zombie child processes that the kernel reassigns to it. Also, it has to keep running, if it crashes or stops, your container stops.</p>

<h2>A brief and wrong introduction to init systems and process supervisors.</h2>
<p><strike>A container can do whatever it wants to do.</strike> Containers should ideally do "one thing"(some people are hyperliteralist who take this to mean a container should have one process... some people are wrong), but that "one thing" probably has some subprocesses to run in order to make it happen. For example, you might have a database running in a container, and part of "running a database" is backing it up, so you might want to run a job to do a dump of the database, and another job to copy those dumps to another computer somewhere else on the internet.</p>

<p>The "init system" on *nix runs as PID 1, it also often starts up services, like your database, webserver, and cron (for instance). Systemd is a (contentious) popular init system that has recently come to prominence on many distros. Upstartd is another one. The classic SysV init system just ran a bunch of shell scripts to start up services. Launchd is the init system on Mac OS X. Dan Bernstein also came up with a set of tools called "daemontools" which decomposed the idea of process management into several utilities instead of one big program, because that's the "Unix way", and of course, this spawned the creation of many clones and systems inspired by daemontools. There are also other process managers like supervisord. All of these are options to run as PID 1 in a container.</p>

<h3>Supervisord</h3>
<p>I happen to like supervisord and I probably using it in the majority of the containers I end up writing. By way of example, here is the config file used in the previous example image:</p>
<pre><code class="ini">[supervisord]
nodaemon=true
childlogdir=/var/opt/example/

[program:nginx]
command=/usr/sbin/nginx
stdout_events_enabled=true
stderr_events_enabled=true

[program:php]
command = /usr/sbin/php5-fpm
autostart=true</code></pre>
</div>



<div class="slide">
<h1>Plumbing</h1>
<h2>Volumes</h2>
<p>Containers are like a running executable. They have state while they are running, but when they are killed all changes to the file system inside the container are erased. Starting a second (or 300th) instance of an image will be completely indepedent of any other instances. This is what is meant when people state that containers are "stateless".</p>

<p>Docker provides a concept called a "Volume" to save state between instances. A volume is simply a folder whose contents are preserved between instance starts (they are associated with a container instance not the image). A volume can be mapped to a folder on the host system or a location inside another container. Volumes can be declared in a Dockerfile with the <strong>VOLUME</strong> directive or on the command line when creating an instance via the <em>-v</em> switch:
<pre><code class="bash">docker run -d --name="example-container" -p 8080:8080 -v ~/example/data:/var/opt/example:rw example</code></pre></p>

<h2>Ports</h2>
<p>In addition to file system state, containers often need to expose ports, the <strong>EXPOSE</strong> directive is for a list of ports the container will be listening on inside the container. These are the ports inside the container though, to connect a port to the outside world you still have use the <em>-p</em> switch when creating the container. The <em>--expose</em> switch can expose additional ports to the ones declared in the Dockerfile. The <em>--link</em> command line switch will attach the container to another container on the same network so you can privately communicate between them.</p>

<h2>Environment Variables</h2>
<p>The <strong>ENV</strong> directive can be used to set environment variables in the container, however, environment variables are also one of the preferred ways of passing runtime configuration into the container via the <em>-e</em> command line switch, which sets the value of environment variables inside the container. (This is especially useful if you're building a <a href="https://12factor.net/">12-factor app</a>.)
</div>



<div class="slide">
<h1>Multiple containers</h1>
<p>Wiring up multiple containers using volumes and links via <code>docker run</code> on the command line gets tedious. Nothing that you can't automate with a good shell script though. You can also use the <a href="https://docs.docker.com/compose/overview/">Docker compose</a> tool to as a more declarative way to describe the relationships between your containers for an application. The command line tool <code>docker-compose</code> looks for a YAML file in the current folder named <em>docker-compose.yml</em>.</p>

<pre><code class="yaml">web:
  image: example_web
  ports:
    - "80:80"
    - "443:443"
  links:
    - database
  env_file: env.sh

database:
  image: example_dbms
  env_file: env.sh</code></pre>
<p>This is an example of a docker compose file for a database backed web application where the database is containerized as well. The <em>env.sh</em> file is just a list of environment variables for configuring the web server and database server. To start the application we can simply do:
<pre><code class="bash">docker-compose up</code></pre>
And docker compose will load the environment variables and start the containers in the proper order.</p>

<p>Docker compose is just one way to do this, and there is really nothing it is doing that you can't also do from the command line with <code>docker</code> directly. Also, it is common to have multiple <em>docker-compose.yml</em> files for a project. For instance, I have ones that configure test environments using containers to emulate backing services like SMTP, and then a separate file for describing a more typical production deployment.</p>
</div>



<div class="slide">
<h1>Deploying to a virtual machine</h1>
<p>So far we've done all of this on our local linux machine. If you are running Docker on OS X or Windows, you will have already encountered <a href="https://docs.docker.com/machine/overview/">Docker machine</a>. Docker machine is a command line tool that can be used to create and control virtual machines and running docker containers. It has drivers cloud services like Digital Ocean and AWS in addition to local VM's like VirtualBox.</p>

<p>To create an virtual machine on Digital Ocean, for instance, you would run the following command:
<pre><code class="bash">docker-machinhttps://hub.docker.com/e create --driver digitalocean --digitalocean-access-token $VPSTOKEN --digitalocean-size 512mb --digitalocean-image ubuntu-16-04-x64 example-vps</code></pre>
You would set the <code>VPSTOKEN</code> environment variable to the API key provided by Digital Ocean. (Yes, running this will cost you money.)</p>

<p>Once the virtual machine is running, to point the local docker client at the docker server listening on the VM, you would run the following command to change the environment variables:
<pre><code class="bash">eval "$(docker-machine env example-vps)"</code></pre></p>

<p>Now all the <code>docker</code> commands you issue will run against the virtual machine you have created.<p>

<h2>Getting images on the virtual machine</h2>
<p>There are several ways to get an image on to a virtual machine. The first is to simply set the docker client pointed to the virtual machine and run a <code>docker build</code> command. Other ways include pulling an image from a docker repository like <a href="https://hub.docker.com/">Docker hub</a>. The third way is to use the <code>docker export</code> and <code>docker import</code> commands to extract the image to a tarball (via export) and restore it on the VM's docker daemon (via import).</p>
</div>



<div class="slide">
<h1>Repeatability</h1>
<p>Dockerfile's allow you to run commands like <code>curl</code> and <code>apt-get</code> while building a container. These commands fetch things from the internet, and as such, you run a risk of creating non-repeatable builds. While apt-get might be better than downloading an arbitrary URL, new versions of packages are released, so without doing any sort of version pinning, you still run risks. In general, Dockerfile's and the build context for an image should be managed in your version control system.</p>

<h1>Registries</h1>
<p>Docker relies on a registry for it's push and pull commands for managing images. However, they have released the code they use to run their registry as open source and you can run a private docker registry for your images and avoid using the main docker hub. In addition, they have already containerized their own registry, so you can deploy it as a containerized application. See <a href="https://github.com/docker/distribution">Docker Distribution</a> for more information.</p>

<p>In addition to docker, it is common to rely on things like apt-get, maven repositories, npm, etc. for building your applications. Registries and artifact repositories exist for these as well, so you can run your own artifact repositories for increased repeatability. <a href="https://www.jfrog.com/artifactory/">Artifactory</a> probably supports most formats you would ever want to use, but you can also run more specialized repositories like <a href="http://www.tecmint.com/apt-cache-server-in-ubuntu/">Apt-cacher-ng</a> for caching apt-get repositories.</p>
</div>



<div class="slide">
<h1>The wider ecosystem</h1>
<p>In addition to docker, docker-compose, and docker-machine there are other linux based containerization tools and cluster management tools for building and deploying containerized applications. One of the downsides of Docker, and Linux containers in general, is that the some parts of the ecosystem are in flux and there are tensions around standards for things like container formats. On the other hand, once you understand Docker, moving to one of the other options should not be difficult (migrating a large investment in one of the options might be more difficult though).</p>

<h2>CoreOS and rkt</h2>
<p>CoreOS is a Linux distro organized around running everything in containers. It uses a tool called <a href="https://coreos.com/rkt/">rkt</a>(pronounced "rocket") instead of Docker. However, <code>rkt</code> can work with Docker's image format, so you can run docker images in addition to <code>rkt</code> generated images. <code>rkt</code> has some compelling features like allowing you to use https to download images rather than relying on a "hub" like Docker does, and it is also based on a daemon running as root, and instead run's the images directly.</p>

<h2>Kubernetes</h2>
<p><a href="http://kubernetes.io/">Kubernetes</a> is a "cluster management tool" for running containerized applications on a collection of servers(either virtual or physical). It is based on internal technology from Google. Applications are described as collections of containers and assigned to servers. Kubernetes can work with both <code>rkt</code> and <code>Docker</code> images.</p>

<h2>Mesos</h2>
<p><a href="http://mesos.apache.org/">Apache Mesos</a> can be used to run containerized applications on a cluster of servers, similar to Kubernetes. The two use different architectures and have different styles, but are essentially in competition.</p>
</div>



<div class="slide">
<h1>Some other usages of containers</h1>
<h2>Desktop applications</h2>
<p>One virtue of a container is that they isolate everything IN the container and keep it from polluting the rest of your system. This can be used to run desktop applications inside containers. Exporting a volume to a folder in your home directory can be done so that you can save files from applications "like normal". The <em>-it</em> command line switch with the <code>docker run</code> command can be used for shell applications. However, since X-Windows uses sockets to communicate between the client and server, you can also run GUI apps inside a container as well. Jessie Frazelle's post: <a href="https://blog.jessfraz.com/post/docker-containers-on-the-desktop/">Docker Containers on the Desktop</a> is a good starting point.</p>

<h2>"Toxic Waste" Containers</h2>
<p>If you don't have to support legacy apps that only work on outdated dependencies then congratulations. For the rest of us, containers also provide a way to manage the risks involved by building an image around the legacy code and outdated dependencies and using that image to isolate the application from the rest of your system.</p>

<h2>Mocking Services</h2>
<p>Once you have an application running in a container, external services like SMTP, databases, etc. will be needed in production to deploy the application. However, in a testing environment, you can create "mock" versions of these services in containers as well to isolate the application entirely from the outside world. For instance, I have images I use that provide a SMTP "sink" that accepts all emails sent, and another image built around running an <a href="http://s3ninja.net/">AWS S3</a> implementation as a mock as well. (I have included the smtp sink in the git repo with these slides.)</p>
</div>



<div class="slide">
	<h1>References and Resources</h1>
	<h2>Docker</h2>
	<ul>
		<li><a href="http://prakhar.me/docker-curriculum/">Docker for Beginners</a></li>
	</ul>

	<h2>Registries</h2>
	<ul>
		<li><a href="https://docs.docker.com/registry/">Docker registry</a></li>
		<li><a href="https://github.com/docker/distribution">Docker Distribution</a>(supercedes Docker registry)</li>
		<li><a href="https://www.jfrog.com/artifactory/">Artifactory</a>(can be used as a docker registry and nearly every other kind of artifact repository)</li>
		<li><a href="https://about.gitlab.com/2016/05/23/gitlab-container-registry/">Gitlab Container Registry</a></li>
		<li><a href="https://hub.docker.com/">Docker hub</a> - The home of official images and public images. Can pay for private repos.</li>
	</ul>

	<h2>Going beyond Docker</h2>
	<ul>
		<li><a href="http://kubernetes.io/">Kubernetes</a></li>
		<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way></a></li>
		<li><a href="http://mesos.apache.org/">Apache Mesos</a></li>
		<li><a href="https://docs.docker.com/swarm/">Docker Swarm</a></li>
		<li><a href="https://shipyard-project.com/">Shipyard</a></li>
		<li><a href="https://aws.amazon.com/ecs/">Amazon ECS</a> - ECS = "EC2 Container Service"</li>
		<li><a href="https://www.terraform.io/">Terraform</a></li>
	</ul>

	<h2>Miscellaneous Docker</h2>
	<ul>
		<li><a href="https://blog.jessfraz.com/post/docker-containers-on-the-desktop/">Docker Containers on the Desktop</a></li>
	</ul>

	<h2>Miscellaneous</h2>
	<ul>
		<li><a href="https://help.ubuntu.com/community/Apt-Cacher-Server">Apt-cacher-server</a></li>
		<li><a href="http://www.tecmint.com/apt-cache-server-in-ubuntu/">Setting up Apt-Cacher-NG</a></li>
		<li><a href="https://docs.docker.com/engine/examples/apt-cacher-ng/">Dockering Apt-Cacher-NG</a></li>
		<li><a href="https://www.aptly.info/">Aptly</a></li>
		<li><a href="https://apt-mirror.github.io/">Apt-mirror</a></li>
		<li><a href="http://www.sonatype.com/download-oss-sonatype">Sonatype Nexus</a></li>
		<li><a href="https://12factor.net/">12 Factor Apps</a></li>
	</ul>

	<h2>Process management and init</h2>
	<ul>
		<li><a href="https://felipec.wordpress.com/2013/11/04/init/">Demystifying init</a></li>
		<li><a href="http://homepage.ntlworld.com/jonathan.deboynepollard/FGA/daemontools-family.html">The daemontools family</a></li>
		<li><a href="http://core.suckless.org/sinit">Suckless init</a></li>
		<li><a href="https://www.freedesktop.org/wiki/Software/systemd/">Systemd</a></li>
		<li><a href="https://en.wikipedia.org/wiki/Launchd">Launchd</a></li>
		<li><a href="http://upstart.ubuntu.com/">upstartd</a></li>
		<li><a href="http://supervisord.org/">Supervisord</a></li>
	</ul>
</div>
</html>
